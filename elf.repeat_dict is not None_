[1mdiff --git a/maskrcnn_benchmark/config/paths_catalog.py b/maskrcnn_benchmark/config/paths_catalog.py[m
[1mindex 0bfcbd5..77e3973 100644[m
[1m--- a/maskrcnn_benchmark/config/paths_catalog.py[m
[1m+++ b/maskrcnn_benchmark/config/paths_catalog.py[m
[36m@@ -7,7 +7,7 @@[m [mimport copy[m
 [m
 class DatasetCatalog(object):[m
     #DATA_DIR = "/home/users/alatif/data/ImageCorpora/"[m
[31m-    DATA_DIR = "/home/cqs/datasets/"[m
[32m+[m[32m    DATA_DIR = "/home1/cqs/datasets/"[m
     DATASETS = {[m
         "coco_2017_train": {[m
             "img_dir": "coco/train2017",[m
[1mdiff --git a/maskrcnn_benchmark/data/datasets/visual_genome.py b/maskrcnn_benchmark/data/datasets/visual_genome.py[m
[1mindex fe6f8d9..921259b 100644[m
[1m--- a/maskrcnn_benchmark/data/datasets/visual_genome.py[m
[1m+++ b/maskrcnn_benchmark/data/datasets/visual_genome.py[m
[36m@@ -4,6 +4,7 @@[m [mimport torch[m
 import h5py[m
 import json[m
 from PIL import Image[m
[32m+[m[32mimport matplotlib.pyplot as plt[m
 import numpy as np[m
 from collections import defaultdict[m
 from tqdm import tqdm[m
[36m@@ -18,6 +19,14 @@[m [mfrom maskrcnn_benchmark.config import cfg[m
 [m
 BOX_SCALE = 1024  # Scale at which we have the boxes[m
 [m
[32m+[m[32m# 51ä¸ªå…³ç³»è°“è¯å æ•°æ®é›†çš„é¢‘çŽ‡ï¼ŒåŒ…å«èƒŒæ™¯[m
[32m+[m[32mREL_PROPORTION = [0,[m[41m [m
[32m+[m[32m                        0.01946, 0.00077, 0.0006, 0.00145, 0.00191, 0.00459, 0.0042, 0.0375, 0.00194, 0.00154,[m[41m [m
[32m+[m[32m                        0.00348, 0.00133, 0.00144, 0.00147, 0.00001, 0.00302, 0.00057, 0.00049, 0.00214, 0.1666,[m[41m [m
[32m+[m[32m                        0.02449, 0.05784, 0.0107, 0.00151, 0.00262, 0.00066, 0.0003, 0.0007, 0.0572, 0.08578,[m[41m [m
[32m+[m[32m                        0.29185, 0.00086, 0.0037, 0.00042, 0.00172, 0.00115, 0.0003, 0.00816, 0.00008, 0.01148,[m[41m [m
[32m+[m[32m                        0.00591, 0.00094, 0.01265, 0.00111, 0.0007, 0.0034, 0.00264, 0.11287, 0.0118, 0.03195][m
[32m+[m
 class VGDataset(torch.utils.data.Dataset):[m
 [m
     def __init__(self, split, img_dir, roidb_file, dict_file, image_file, transforms=None,[m
[36m@@ -72,7 +81,7 @@[m [mclass VGDataset(torch.utils.data.Dataset):[m
             self.img_info = [self.img_info[i] for i in np.where(self.split_mask)[0]][m
             self.idx_list = list(range(len(self.filenames)))[m
 [m
[31m-        # å…¬å¼ å›¾åƒçº§è¿‡é‡‡æ ·[m
[32m+[m[32m        # ========== å›¾åƒçº§è¿‡é‡‡æ · ==========[m
         if self.split == 'train' and cfg.MODEL.ROI_RELATION_HEAD.DATA_RESAMPLE:[m
             self.resampling_method = 'bilvl'[m
             self.global_rf = 0.07[m
[36m@@ -87,12 +96,59 @@[m [mclass VGDataset(torch.utils.data.Dataset):[m
             synchronize()[m
             self.repeat_dict = resampling_dict_generation(self, self.ind_to_predicates)[m
 [m
[31m-            duplicate_idx_list = [][m
[32m+[m[32m            # è¿‡é‡‡æ ·åŽæ¯å¼ å›¾ç‰‡è®°å½•å”¯ä¸€çš„ç´¢å¼•[m
[32m+[m[32m            copy_idx_list = [idx for idx in range(len(self.filenames))][m
[32m+[m[32m            # è®°å½•è¿‡é‡‡æ ·åŽæ¯å¼ å›¾ç‰‡å¯¹åº”çš„åŽŸå§‹å›¾ç‰‡ç´¢å¼•[m
[32m+[m[32m            final_to_origin = [idx for idx in range(len(self.filenames))][m
             for idx in range(len(self.filenames)):[m
                 r_c = self.repeat_dict[idx][m
[31m-                duplicate_idx_list.extend([idx for _ in range(r_c)])[m
[31m-            self.idx_list = duplicate_idx_list[m
[31m-        # å…¬å¼ å›¾åƒçº§è¿‡é‡‡æ · ç»“æŸ[m
[32m+[m[32m                for _ in range(r_c - 1):[m
[32m+[m[32m                    copy_idx_list.append(copy_idx_list[-1] + 1)[m
[32m+[m[32m                    final_to_origin.append(idx)[m
[32m+[m[32m            self.idx_list = copy_idx_list[m
[32m+[m[32m            self.final_to_origin = final_to_origin[m
[32m+[m[32m        # ========== å›¾åƒçº§è¿‡é‡‡æ · - ç»“æŸ ==========[m
[32m+[m
[32m+[m[32m        # ========== åœºæ™¯å›¾çº§å›¾åƒè£å‰ª-1 ==========[m
[32m+[m[32m        # è®°å½•æ¯å¼ å›¾ç‰‡çš„è£å‰ªåŒºåŸŸ[m
[32m+[m[32m        crop_area_list = [][m
[32m+[m[32m        for idx in tqdm(self.idx_list):[m
[32m+[m[32m            relation = torch.from_numpy(self.relationships[final_to_origin[idx]])[m
[32m+[m[32m            # æŒ‰ç…§è°“è¯å…³ç³»é¢‘çŽ‡è¿›è¡Œä¸¢å¼ƒ[m
[32m+[m[32m            freq_pre_img = [REL_PROPORTION[i] for i in relation[:, -1]][m
[32m+[m[32m            drop_rate = torch.tensor(np.divide(freq_pre_img, np.sum(freq_pre_img)))[m
[32m+[m[32m            if_drop = drop_rate > torch.rand(drop_rate.shape).type(torch.float64)[m
[32m+[m[32m            relation_keep = relation[if_drop][m
[32m+[m[32m            # æ‹¿åˆ°å…³ç³»å¯¹åº”çš„box[m
[32m+[m[32m            w, h = self.get_img_info(final_to_origin[idx])['width'], self.get_img_info(final_to_origin[idx])['height'][m
[32m+[m[32m            box = self.gt_boxes[final_to_origin[idx]] / BOX_SCALE * max(w, h)[m
[32m+[m[32m            box = torch.from_numpy(box).reshape(-1, 4)  # guard against no boxes[m
[32m+[m[32m            box_select = torch.zeros(box.shape[0]).type(torch.BoolTensor)[m
[32m+[m[32m            for tgt_head_idx in relation[:, 0]:[m
[32m+[m[32m                box_select[tgt_head_idx] = True[m
[32m+[m[32m            for tgt_tail_idx in relation[:, 1]:[m
[32m+[m[32m                box_select[tgt_tail_idx] = True[m
[32m+[m[32m            # æ‰¾å‡ºæ‰€é€‰boxçš„æœ€å·¦ä¸Šçš„ç‚¹å’Œæœ€å³ä¸‹çš„ç‚¹[m
[32m+[m[32m            if len(box) == 0 or len(relation_keep) == 0:[m
[32m+[m[32m                left = 0[m
[32m+[m[32m                top = 0[m
[32m+[m[32m                right = w[m
[32m+[m[32m                bottom = h[m
[32m+[m[32m            else:[m
[32m+[m[32m                left = int(min(box[box_select, 0]).item())[m
[32m+[m[32m                top = int(min(box[box_select, 1]).item())[m
[32m+[m[32m                right = min(int(max(box[box_select, 2]).item()) + 1, w)[m
[32m+[m[32m                bottom = min(int(max(box[box_select, 3]).item()) + 1, h)[m
[32m+[m[32m            # print('----- è¢«ç­›é€‰åŽçš„å…³ç³»æ¶‰åŠåˆ°çš„ä¸»è°“å®¾box -----')[m
[32m+[m[32m            # print(target.bbox[box_select])[m
[32m+[m[32m            # åœ¨å›¾ç‰‡å·¦ä¸Šè§’ï¼ˆå³ä¸‹è§’ï¼‰å’Œboxæœ€å°è¦†ç›–åŒºåŸŸæœ€å·¦ä¸Šè§’ï¼ˆå³ä¸‹è§’ï¼‰æ‰§è¡Œéšæœºè£å‰ª[m
[32m+[m[32m            x1 = random.randint(0, left)[m
[32m+[m[32m            y1 = random.randint(0, top)[m
[32m+[m[32m            x2 = random.randint(right, w)[m
[32m+[m[32m            y2 = random.randint(bottom, h)[m
[32m+[m[32m            crop_area_list.append([x1, y1, x2, y2])[m
[32m+[m[32m        self.crop_area_list = crop_area_list[m
[32m+[m[32m        # ========== åœºæ™¯å›¾çº§å›¾åƒè£å‰ª-1 - ç»“æŸ ==========[m
 [m
 [m
     def __getitem__(self, index):[m
[36m@@ -100,7 +156,7 @@[m [mclass VGDataset(torch.utils.data.Dataset):[m
         #    while(random.random() > self.img_info[index]['anti_prop']):[m
         #        index = int(random.random() * len(self.filenames))[m
         if self.repeat_dict is not None:[m
[31m-                    index = self.idx_list[index][m
[32m+[m[32m            index = self.idx_list[index][m
         [m
         if self.custom_eval:[m
             img = Image.open(self.custom_files[index]).convert("RGB")[m
[36m@@ -117,9 +173,118 @@[m [mclass VGDataset(torch.utils.data.Dataset):[m
         [m
         target = self.get_groundtruth(index, flip_img)[m
 [m
[32m+[m[32m        # åœºæ™¯å›¾çº§å›¾åƒè£å‰ª ----- å¼€å§‹[m
[32m+[m[32m        # 51ä¸ªå…³ç³»è°“è¯å æ•°æ®é›†çš„é¢‘çŽ‡ï¼ŒåŒ…å«èƒŒæ™¯[m
[32m+[m[41m        [m
[32m+[m[32m        dict_file = '/home1/cqs/datasets/vg/VG-SGG-dicts-with-attri.json'[m
[32m+[m[32m        ind_to_classes, ind_to_predicates, _ = load_info(dict_file)[m
[32m+[m[32m        num_box = len(target.bbox)[m
[32m+[m[32m        # èŽ·å–å¯¹è±¡æ ‡ç­¾[m
[32m+[m[32m        obj_labels_id = target.get_field('labels')[m
[32m+[m[32m        obj_labels = [][m
[32m+[m[32m        for j in obj_labels_id:[m
[32m+[m[32m            obj_labels.append(ind_to_classes[j])[m
[32m+[m[32m        # print('\n' + '=' * 50)[m
[32m+[m
[32m+[m[32m        # å‘½ä»¤è¡Œè¾“å‡ºå…³ç³»å¯¹[m
[32m+[m[32m        # èŽ·å–å…³ç³» [ä¸»è¯­ç´¢å¼•, å®¾è¯­ç´¢å¼•, å…³ç³»æ ‡ç­¾][m
[32m+[m[32m        tgt_rel_matrix = target.get_field("relation")[m
[32m+[m[32m        tgt_pair_idxs = torch.nonzero(tgt_rel_matrix > 0)[m
[32m+[m[32m        tgt_head_idxs = tgt_pair_idxs[:, 0].contiguous().view(-1)[m
[32m+[m[32m        tgt_tail_idxs = tgt_pair_idxs[:, 1].contiguous().view(-1)[m
[32m+[m[32m        tgt_rel_labs = tgt_rel_matrix[tgt_head_idxs, tgt_tail_idxs].contiguous().view(-1)[m
[32m+[m[32m        relation = torch.stack([tgt_head_idxs, tgt_tail_idxs, tgt_rel_labs], 1)[m
[32m+[m[32m        if relation.shape[0] > 0:[m
[32m+[m[32m            tgt_head_idxs = relation[:, 0].contiguous().view(-1)[m
[32m+[m[32m            tgt_tail_idxs = relation[:, 1].contiguous().view(-1)[m
[32m+[m[32m            tgt_rel_labs = relation[:, 2].contiguous().view(-1)[m
[32m+[m[32m            # print('-------- åŽŸå§‹å…³ç³» --------')[m
[32m+[m[32m            # for k in range(relation.shape[0]):[m
[32m+[m[32m            #     print("box[{}]  {} --{}--> {}  box[{}]".format(tgt_head_idxs[k],[m
[32m+[m[32m            #                                                    obj_labels[tgt_head_idxs[k]],[m
[32m+[m[32m            #                                                    ind_to_predicates[tgt_rel_labs[k]],[m
[32m+[m[32m            #                                                    obj_labels[tgt_tail_idxs[k]],[m
[32m+[m[32m            #                                                    tgt_tail_idxs[k][m
[32m+[m[32m            #                                                    ))[m
[32m+[m[32m            # æŒ‰ç…§è°“è¯å…³ç³»é¢‘çŽ‡è¿›è¡Œä¸¢å¼ƒ[m
[32m+[m[32m            freq_pre_img = [REL_PROPORTION[i] for i in tgt_rel_labs][m
[32m+[m[32m            drop_rate = torch.tensor(np.divide(freq_pre_img, np.sum(freq_pre_img)))[m
[32m+[m[32m            if_drop = drop_rate > torch.rand(drop_rate.shape).type(torch.float64)[m
[32m+[m[32m            relation_keep = relation[if_drop][m
[32m+[m[32m            # print('-------- æ‰§è¡Œä¸¢å¼ƒåŽçš„å…³ç³» --------')[m
[32m+[m[32m            tgt_head_idxs = relation_keep[:, 0].contiguous().view(-1)[m
[32m+[m[32m            tgt_tail_idxs = relation_keep[:, 1].contiguous().view(-1)[m
[32m+[m[32m            tgt_rel_labs = relation_keep[:, 2].contiguous().view(-1)[m
[32m+[m[32m            # for k in range(relation_keep.shape[0]):[m
[32m+[m[32m            #     print("box[{}]  {} --{}--> {}  box[{}]".format(tgt_head_idxs[k],[m
[32m+[m[32m            #                                                    obj_labels[tgt_head_idxs[k]],[m
[32m+[m[32m            #                                                    ind_to_predicates[tgt_rel_labs[k]],[m
[32m+[m[32m            #                                                    obj_labels[tgt_tail_idxs[k]],[m
[32m+[m[32m            #                                                    tgt_tail_idxs[k][m
[32m+[m[32m            #                                                    ))[m
[32m+[m[32m            # æ‹¿åˆ°å…³ç³»å¯¹åº”çš„box[m
[32m+[m[32m            box_select = torch.zeros(target.bbox.shape[0]).type(torch.BoolTensor)[m
[32m+[m[32m            for tgt_head_idx in tgt_head_idxs:[m
[32m+[m[32m                box_select[tgt_head_idx] = True[m
[32m+[m[32m            for tgt_tail_idx in tgt_tail_idxs:[m
[32m+[m[32m                box_select[tgt_tail_idx] = True[m
[32m+[m[32m            # æ‰¾å‡ºæ‰€é€‰boxçš„æœ€å·¦ä¸Šçš„ç‚¹å’Œæœ€å³ä¸‹çš„ç‚¹[m
[32m+[m[32m            if len(target.bbox) == 0 or len(relation_keep) == 0:[m
[32m+[m[32m                left = 0[m
[32m+[m[32m                top = 0[m
[32m+[m[32m                right = img.size[0][m
[32m+[m[32m                bottom = img.size[1][m
[32m+[m[32m            else:[m
[32m+[m[32m                left = int(min(target.bbox[box_select, 0]).item())[m
[32m+[m[32m                top = int(min(target.bbox[box_select, 1]).item())[m
[32m+[m[32m                right = min(int(max(target.bbox[box_select, 2]).item()) + 1, img.size[0])[m
[32m+[m[32m                bottom = min(int(max(target.bbox[box_select, 3]).item()) + 1, img.size[1])[m
[32m+[m[32m            # print('----- è¢«ç­›é€‰åŽçš„å…³ç³»æ¶‰åŠåˆ°çš„ä¸»è°“å®¾box -----')[m
[32m+[m[32m            # print(target.bbox[box_select])[m
[32m+[m[32m            # åœ¨å›¾ç‰‡å·¦ä¸Šè§’ï¼ˆå³ä¸‹è§’ï¼‰å’Œboxæœ€å°è¦†ç›–åŒºåŸŸæœ€å·¦ä¸Šè§’ï¼ˆå³ä¸‹è§’ï¼‰æ‰§è¡Œéšæœºè£å‰ª[m
[32m+[m[32m            x1 = random.randint(0, left)[m
[32m+[m[32m            y1 = random.randint(0, top)[m
[32m+[m[32m            x2 = random.randint(right, img.size[0])[m
[32m+[m[32m            y2 = random.randint(bottom, img.size[1])[m
[32m+[m[32m            # print('----- æ‰§è¡Œè£å‰ªçš„åŒºåŸŸ -----')[m
[32m+[m[32m            # print(f"{x1}, {y1}, {x2}, {y2}")[m
[32m+[m[32m            # æ²¡æœ‰å®Œå…¨è¢«è£å‰ªåŒºåŸŸåŒ…æ‹¬çš„boxä¸€å¾‹åŽ»é™¤[m
[32m+[m[32m            idx = 0[m
[32m+[m[32m            for box in target.bbox:[m
[32m+[m[32m                if box[0] > x1 and box[1] > y1 and box[2] < x2 and box[3] < y2:[m
[32m+[m[32m                    box_select[idx] = True[m
[32m+[m[32m                idx += 1[m
[32m+[m[32m            # å¦‚æžœåŽ»é™¤å…³ç³»åŠå…¶å¯¹åº”çš„ä¸»å®¾è¯­çœŸå®žæ¡†åŽä¸å­˜åœ¨çœŸå®žæ¡†ï¼Œç›´æŽ¥è¿”å›žåŽŸæ¥çš„å›¾ç‰‡å’Œæ³¨è§£ï¼Œå¦åˆ™ä¼šæŠ¥é”™[m
[32m+[m[32m            if True in box_select:[m
[32m+[m[32m                # æ‰§è¡Œè£å‰ª[m
[32m+[m[32m                img = img.crop((x1, y1, x2, y2))[m
[32m+[m[32m                # print('----- æœ€ç»ˆè¢«ä¿ç•™çš„box -----')[m
[32m+[m[32m                # for box_idx in range(num_box):[m
[32m+[m[32m                #     if box_select[box_idx] == True:[m
[32m+[m[32m                #         print(f"box[{box_idx}]: {obj_labels[box_idx]} {target.bbox[box_idx]}")[m
[32m+[m
[32m+[m[32m                # == æ³¨é‡Šä¸Šçš„æ“ä½œ ==[m
[32m+[m[32m                # box åæ ‡éœ€è¦éšè£å‰ªå›¾ç‰‡å¤§å°è¿›è¡Œç­‰æ¯”ä¾‹è°ƒæ•´[m
[32m+[m[32m                coordinate_change = torch.tensor([x1, y1, x1, y1])[m
[32m+[m[32m                target.bbox = target.bbox[box_select] - coordinate_change[m
[32m+[m[32m                # åŽ»é™¤æ²¡æœ‰è¢«é€‰ä¸­çš„ box æ ‡ç­¾[m
[32m+[m[32m                target.add_field("labels", target.get_field("labels")[box_select])[m
[32m+[m[32m                # target.relation çŸ©é˜µåŽ»é™¤æ²¡æœ‰è¢«é€‰ä¸­çš„å…³ç³»[m
[32m+[m[32m                relation_drop = relation[~if_drop][m
[32m+[m[32m                target.get_field("relation")[relation_drop[:, 0], relation_drop[:, 1]] = 0[m
[32m+[m[32m                # ä¿®æ”¹ target æ³¨é‡Šä¸­å›¾ç‰‡çš„å¤§å°[m
[32m+[m[32m                target.size = (x2 - x1, y2 - y1)[m
[32m+[m[32m                # æ‰§è¡Œä¸¢å¼ƒboxåŽï¼Œå…³ç³»å¯¹åº”çš„boxåºå·å¾—æ”¹å˜[m
[32m+[m[32m                target.add_field("relation", target.get_field("relation")[box_select][:, box_select])[m
[32m+[m[32m                # ä»ŽåŽŸæ•°æ®é›†ä¸­èŽ·å–çš„ä¿¡æ¯ä¹Ÿå¾—æ”¹[m
[32m+[m[32m                self.gt_boxes[index][m
[32m+[m[32m        # åœºæ™¯å›¾çº§å›¾åƒè£å‰ª ----- ç»“æŸ[m
[32m+[m
         if flip_img:[m
             img = img.transpose(method=Image.FLIP_LEFT_RIGHT)[m
 [m
[32m+[m[32m        # æ‰§è¡Œå›¾åƒå¢žå¼ºï¼ŒåŒ…æ‹¬ï¼š[m
[32m+[m[32m        # 1. äº®åº¦å˜æ¢ 2. resize 3. åž‚ç›´ç¿»è½¬ 4. æ°´å¹³ç¿»è½¬ 5. è½¬æ¢ä¸ºtensor 6. å½’ä¸€åŒ–[m
         if self.transforms is not None:[m
             img, target = self.transforms(img, target)[m
 [m
[36m@@ -164,13 +329,14 @@[m [mclass VGDataset(torch.utils.data.Dataset):[m
         # it will take a while, you only need to do it once[m
 [m
         # correct_img_info(self.img_dir, self.image_file)[m
[31m-        return self.img_info[index][m
[32m+[m[32m        return self.img_info[self.final_to_origin[index]][m
 [m
     def get_groundtruth(self, index, evaluation=False, flip_img=False):[m
[31m-        img_info = self.get_img_info(index)[m
[32m+[m[32m        img_info = self.get_img_info(self.final_to_origin[index])[m
         w, h = img_info['width'], img_info['height'][m
         # important: recover original box from BOX_SCALE[m
[31m-        box = self.gt_boxes[index] / BOX_SCALE * max(w, h)[m
[32m+[m[32m        # VG-SGG-with-attri.h5 æ–‡ä»¶ä¿å­˜çš„æ˜¯å›¾ç‰‡ resize åˆ° 1024 æ—¶å€™çš„ boxï¼Œå› æ­¤éœ€è¦å°†æ–‡ä»¶ä¸­çš„boxæ•°æ®æ¢å¤ä¸ºåŽŸå§‹æ•°æ®[m
[32m+[m[32m        box = self.gt_boxes[self.final_to_origin[index]] / BOX_SCALE * max(w, h)[m
         box = torch.from_numpy(box).reshape(-1, 4)  # guard against no boxes[m
         if flip_img:[m
             new_xmin = w - box[:,2][m
[36m@@ -193,11 +359,11 @@[m [mclass VGDataset(torch.utils.data.Dataset):[m
             relation = [(k[0], k[1], np.random.choice(v)) for k,v in all_rel_sets.items()][m
             relation = np.array(relation, dtype=np.int32)[m
         [m
[31m-        # å…¬å¼ åº”ç”¨é‡é‡‡æ ·[m
[32m+[m[32m        # ========== å®žä¾‹çº§æ¬ é‡‡æ · ==========[m
         if self.repeat_dict is not None:[m
             # è¿‡æ»¤æŽ‰å¤´éƒ¨å…³ç³»åŽçš„å…³ç³», åŽŸå§‹/è¿‡æ»¤å‰çš„å…³ç³»[m
             relation, _ = apply_resampling(index, relation, self.repeat_dict, self.drop_rate, )[m
[31m-        # å…¬å¼ åº”ç”¨é‡é‡‡æ ·ç»“æŸ[m
[32m+[m[32m        # ========== å®žä¾‹çº§æ¬ é‡‡æ · - ç»“æŸ ==========[m
 [m
         # add relation to target[m
         num_box = len(target)[m
[1mdiff --git a/maskrcnn_benchmark/utils/imports.py b/maskrcnn_benchmark/utils/imports.py[m
[1mindex 53e27e2..4d2148f 100644[m
[1m--- a/maskrcnn_benchmark/utils/imports.py[m
[1m+++ b/maskrcnn_benchmark/utils/imports.py[m
[36m@@ -1,7 +1,7 @@[m
 # Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.[m
 import torch[m
 [m
[31m-if torch._six.PY3:[m
[32m+[m[32mif torch._six.PY37:[m
     import importlib[m
     import importlib.util[m
     import sys[m
[1mdiff --git a/scripts/train_motif.sh b/scripts/train_motif.sh[m
[1mindex 453d1fe..571f9b1 100644[m
[1m--- a/scripts/train_motif.sh[m
[1m+++ b/scripts/train_motif.sh[m
[36m@@ -14,10 +14,10 @@[m [mSOLVER.CHECKPOINT_PERIOD 2000 \[m
 SOLVER.PRE_VAL False \[m
 \[m
 MODEL.ROI_RELATION_HEAD.DATA_RESAMPLE True \[m
[31m-MODEL.ROI_RELATION_HEAD.RESAMPLE_METHOD bi-level-resample \[m
[32m+[m[32mMODEL.ROI_RELATION_HEAD.RESAMPLE_METHOD heterophily-resample \[m
 \[m
[31m-GLOVE_DIR /root/autodl-tmp/glove \[m
[31m-MODEL.PRETRAINED_DETECTOR_CKPT /root/autodl-tmp/pretrained_faster_rcnn/model_final.pth \[m
[31m-OUTPUT_DIR /root/autodl-tmp/checkpoints/motif-precls-biresample[m
[32m+[m[32mGLOVE_DIR /home1/cqs/glove \[m
[32m+[m[32mMODEL.PRETRAINED_DETECTOR_CKPT /home1/cqs/checkpoints/pretrained_faster_rcnn/model_final.pth \[m
[32m+[m[32mOUTPUT_DIR /home1/cqs/checkpoints/motif-precls[m
 [m
 # shutdown[m
\ No newline at end of file[m
[1mdiff --git a/tools/relation_train_net.py b/tools/relation_train_net.py[m
[1mindex bc1dc26..eea8580 100644[m
[1m--- a/tools/relation_train_net.py[m
[1m+++ b/tools/relation_train_net.py[m
[36m@@ -51,7 +51,7 @@[m [mnp.random.seed(SEED)[m
 # torch.backends.cudnn.benchmark = False  # é»˜è®¤ä¸ºFalse[m
 torch.backends.cudnn.deterministic = True  # é»˜è®¤ä¸ºFalse;benchmarkä¸ºTrueæ—¶,yè¦æŽ’é™¤éšæœºæ€§å¿…é¡»ä¸ºTrue[m
 [m
[31m-# torch.autograd.set_detect_anomaly(True)[m
[32m+[m[32mtorch.autograd.set_detect_anomaly(True)[m
 [m
 def train(cfg, local_rank, distributed, logger):[m
     debug_print(logger, 'prepare training')[m
[1mdiff --git a/z-joson/visualize_relationship.py b/z-joson/visualize_relationship.py[m
[1mindex 76ebb78..81c5851 100644[m
[1m--- a/z-joson/visualize_relationship.py[m
[1m+++ b/z-joson/visualize_relationship.py[m
[36m@@ -18,6 +18,14 @@[m [mfrom maskrcnn_benchmark.structures.boxlist_ops import boxlist_iou[m
 [m
 BOX_SCALE = 1024  # Scale at which we have the boxes[m
 [m
[32m+[m[32m# 51ä¸ªå…³ç³»è°“è¯å æ•°æ®é›†çš„é¢‘çŽ‡ï¼ŒåŒ…å«èƒŒæ™¯[m[41m[m
[32m+[m[32mREL_PROPORTION = [0,[m[41m [m
[32m+[m[32m                    0.01946, 0.00077, 0.0006, 0.00145, 0.00191, 0.00459, 0.0042, 0.0375, 0.00194, 0.00154,[m[41m [m
[32m+[m[32m                    0.00348, 0.00133, 0.00144, 0.00147, 0.00001, 0.00302, 0.00057, 0.00049, 0.00214, 0.1666,[m[41m [m
[32m+[m[32m                    0.02449, 0.05784, 0.0107, 0.00151, 0.00262, 0.00066, 0.0003, 0.0007, 0.0572, 0.08578,[m[41m [m
[32m+[m[32m                    0.29185, 0.00086, 0.0037, 0.00042, 0.00172, 0.00115, 0.0003, 0.00816, 0.00008, 0.01148,[m[41m [m
[32m+[m[32m                    0.00591, 0.00094, 0.01265, 0.00111, 0.0007, 0.0034, 0.00264, 0.11287, 0.0118, 0.03195][m[41m[m
[32m+[m[41m[m
 [m
 class VGDataset(torch.utils.data.Dataset):[m
 [m
[36m@@ -312,7 +320,7 @@[m [mclass VGDataset(torch.utils.data.Dataset):[m
 # å¯è§†åŒ–åœºæ™¯å›¾[m
 def visualize_vg():[m
     dataset_name = 'VG_stanford_filtered_with_attribute_train'[m
[31m-    dict_file = '/home/cqs/datasets/vg/VG-SGG-dicts-with-attri.json'[m
[32m+[m[32m    dict_file = '/home1/cqs/datasets/vg/VG-SGG-dicts-with-attri.json'[m[41m[m
     dataset_catalog = paths_catalog.DatasetCatalog[m
     data = dataset_catalog.get(dataset_name, cfg)[m
     args = data["args"][m
[36m@@ -566,7 +574,168 @@[m [mdef get_redundancy():[m
     print(min_heterophily_3)[m
     print(min_heterophily_4)[m
 [m
[32m+[m[41m[m
[32m+[m[32m# è£å‰ªåœºæ™¯å›¾[m[41m[m
[32m+[m[32mdef clip_vg():[m[41m[m
[32m+[m[32m    # åŸºç¡€å˜é‡[m[41m[m
[32m+[m[32m    dataset_name = 'VG_stanford_filtered_with_attribute_train'[m[41m[m
[32m+[m[32m    dict_file = '/home1/cqs/datasets/vg/VG-SGG-dicts-with-attri.json'[m[41m[m
[32m+[m[32m    dataset_catalog = paths_catalog.DatasetCatalog[m[41m[m
[32m+[m[32m    data = dataset_catalog.get(dataset_name, cfg)[m[41m[m
[32m+[m[32m    args = data["args"][m[41m[m
[32m+[m[32m    # åŽ»é™¤æŽ‰ä¸€ä¸ªkey[m[41m[m
[32m+[m[32m    if "capgraphs_file" in args.keys():[m[41m[m
[32m+[m[32m            del args["capgraphs_file"][m[41m[m
[32m+[m[32m    dataset = VGDataset(**args)[m[41m[m
[32m+[m[32m    ind_to_classes, ind_to_predicates, _ = dataset.load_info(dict_file)[m[41m[m
[32m+[m[41m[m
[32m+[m[32m    # å¯¹äºŽæ•°æ®é›†ä¸­çš„æ¯ä¸€å¼ å›¾ç‰‡[m[41m[m
[32m+[m[32m    for i in range(len(dataset)):[m[41m[m
[32m+[m[32m        anno = dataset.get_groundtruth(i)[m[41m[m
[32m+[m[32m        img_path = dataset.filenames[i][m[41m[m
[32m+[m[32m        img = cv2.imread(img_path)[m[41m[m
[32m+[m[32m        num_box = len(anno.bbox)[m[41m[m
[32m+[m[32m        # èŽ·å–å¯¹è±¡æ ‡ç­¾[m[41m[m
[32m+[m[32m        obj_labels_id = anno.get_field('labels')[m[41m[m
[32m+[m[32m        obj_labels = [][m[41m[m
[32m+[m[32m        for j in obj_labels_id:[m[41m[m
[32m+[m[32m            obj_labels.append(ind_to_classes[j])[m[41m[m
[32m+[m[32m        # å¯¹è±¡è¾¹æ¡†æ ‡ç­¾å¯è§†åŒ–[m[41m[m
[32m+[m[32m        box_color = (255, 0, 255)[m[41m[m
[32m+[m[32m        print('\n' + '=' * 50)[m[41m[m
[32m+[m[41m[m
[32m+[m[32m        # å¯¹äºŽå›¾ç‰‡ä¸­æ¯ä¸€ä¸ªbox[m[41m[m
[32m+[m[32m        for j in range(num_box):[m[41m[m
[32m+[m[32m            box = anno.bbox[j][m[41m[m
[32m+[m[32m            x1, y1, x2, y2 = box[:4][m[41m[m
[32m+[m[32m            x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)[m[41m[m
[32m+[m[32m            # ä¸ºæ¯ä¸ªè¾¹æ¡†æ·»åŠ å¯¹è±¡æ ‡ç­¾[m[41m[m
[32m+[m[32m            obj_label = str(j) + ' ' + obj_labels[j][m[41m[m
[32m+[m[32m            label_size = cv2.getTextSize(obj_label + '0', cv2.FONT_HERSHEY_SIMPLEX, 0.5, 2)[0][m[41m[m
[32m+[m[32m            if y1 - label_size[1] - 3 < 0:[m[41m[m
[32m+[m[32m                cv2.putText(img, obj_label, (x1, y1 + label_size[1] + 3), cv2.FONT_HERSHEY_SIMPLEX,[m[41m[m
[32m+[m[32m                            0.5, (41, 226, 243), thickness=2)[m[41m[m
[32m+[m[32m            else:[m[41m[m
[32m+[m[32m                cv2.putText(img, obj_label, (x1, y1 - 3), cv2.FONT_HERSHEY_SIMPLEX,[m[41m[m
[32m+[m[32m                            0.5, (41, 226, 243), thickness=2)[m[41m[m
[32m+[m[32m            # æ˜¾ç¤ºå¯¹è±¡è¾¹æ¡†[m[41m[m
[32m+[m[32m            cv2.rectangle(img, (x1, y1), (x2, y2), color=box_color, thickness=2)[m[41m[m
[32m+[m[32m            # å‘½ä»¤è¡Œè¾“å‡ºè¾¹æ¡†ä¿¡æ¯[m[41m[m
[32m+[m[32m            print("box[{}]: {}".format(j, obj_labels[j]))[m[41m[m
[32m+[m[32m        # æ˜¾ç¤º[m[41m[m
[32m+[m[32m        cv2.imshow("draw_0", img)[m[41m[m
[32m+[m[32m        cv2.waitKey(0)[m[41m[m
[32m+[m[41m[m
[32m+[m[32m        # å‘½ä»¤è¡Œè¾“å‡ºå…³ç³»å¯¹[m[41m[m
[32m+[m[32m        relation = anno.get_field('relation_tuple')[m[41m[m
[32m+[m[32m        if relation.shape[0] > 0:[m[41m[m
[32m+[m[32m            tgt_head_idxs = relation[:, 0].contiguous().view(-1)[m[41m[m
[32m+[m[32m            tgt_tail_idxs = relation[:, 1].contiguous().view(-1)[m[41m[m
[32m+[m[32m            tgt_rel_labs = relation[:, 2].contiguous().view(-1)[m[41m[m
[32m+[m[32m            print('-------- åŽŸå§‹å…³ç³» --------')[m[41m[m
[32m+[m[32m            for k in range(relation.shape[0]):[m[41m[m
[32m+[m[32m                print("box[{}]  {} --{}--> {}  box[{}]".format(tgt_head_idxs[k],[m[41m[m
[32m+[m[32m                                                               obj_labels[tgt_head_idxs[k]],[m[41m[m
[32m+[m[32m                                                               ind_to_predicates[tgt_rel_labs[k]],[m[41m[m
[32m+[m[32m                                                               obj_labels[tgt_tail_idxs[k]],[m[41m[m
[32m+[m[32m                                                               tgt_tail_idxs[k][m[41m[m
[32m+[m[32m                                                               ))[m[41m[m
[32m+[m[32m            # æŒ‰ç…§è°“è¯å…³ç³»é¢‘çŽ‡è¿›è¡Œä¸¢å¼ƒ[m[41m[m
[32m+[m[32m            freq_pre_img = [REL_PROPORTION[i] for i in tgt_rel_labs][m[41m[m
[32m+[m[32m            drop_rate = torch.tensor(np.divide(freq_pre_img, np.sum(freq_pre_img)))[m[41m[m
[32m+[m[32m            if_drop = drop_rate > torch.rand(drop_rate.shape)[m[41m[m
[32m+[m[32m            relation = relation[if_drop][m[41m[m
[32m+[m[32m            print('-------- æ‰§è¡Œä¸¢å¼ƒåŽçš„å…³ç³» --------')[m[41m[m
[32m+[m[32m            tgt_head_idxs = relation[:, 0].contiguous().view(-1)[m[41m[m
[32m+[m[32m            tgt_tail_idxs = relation[:, 1].contiguous().view(-1)[m[41m[m
[32m+[m[32m            tgt_rel_labs = relation[:, 2].contiguous().view(-1)[m[41m[m
[32m+[m[32m            for k in range(relation.shape[0]):[m[41m[m
[32m+[m[32m                print("box[{}]  {} --{}--> {}  box[{}]".format(tgt_head_idxs[k],[m[41m[m
[32m+[m[32m                                                               obj_labels[tgt_head_idxs[k]],[m[41m[m
[32m+[m[32m                                                               ind_to_predicates[tgt_rel_labs[k]],[m[41m[m
[32m+[m[32m                                                               obj_labels[tgt_tail_idxs[k]],[m[41m[m
[32m+[m[32m                                                               tgt_tail_idxs[k][m[41m[m
[32m+[m[32m                                                               ))[m[41m[m
[32m+[m[32m            # æ‹¿åˆ°å…³ç³»å¯¹åº”çš„box[m[41m[m
[32m+[m[32m            box_select = torch.zeros(anno.bbox.shape[0]).type(torch.BoolTensor)[m[41m[m
[32m+[m[32m            for tgt_head_idx in tgt_head_idxs:[m[41m[m
[32m+[m[32m                box_select[tgt_head_idx] = True[m[41m[m
[32m+[m[32m            for tgt_tail_idx in tgt_tail_idxs:[m[41m[m
[32m+[m[32m                box_select[tgt_tail_idx] = True[m[41m[m
[32m+[m[32m            # é‡ç½®å›¾ç‰‡[m[41m[m
[32m+[m[32m            img = cv2.imread(img_path)[m[41m[m
[32m+[m[32m            # æ‰¾å‡ºæ‰€æœ‰boxçš„æœ€å·¦ä¸Šçš„ç‚¹å’Œæœ€å³ä¸‹çš„ç‚¹[m[41m[m
[32m+[m[32m            if len(anno.bbox) == 0 or len(relation) == 0:[m[41m[m
[32m+[m[32m                left = 0[m[41m[m
[32m+[m[32m                top = 0[m[41m[m
[32m+[m[32m                right = img.shape[1][m[41m[m
[32m+[m[32m                bottom = img.shape[0][m[41m[m
[32m+[m[32m            else:[m[41m[m
[32m+[m[32m                left = int(min(anno.bbox[box_select, 0]).item())[m[41m[m
[32m+[m[32m                top = int(min(anno.bbox[box_select, 1]).item())[m[41m[m
[32m+[m[32m                right = int(max(anno.bbox[box_select, 2]).item())[m[41m[m
[32m+[m[32m                bottom = int(max(anno.bbox[box_select, 3]).item())[m[41m[m
[32m+[m[32m            print('----- è¢«ç­›é€‰åŽçš„å…³ç³»æ¶‰åŠåˆ°çš„ä¸»è°“å®¾box -----')[m[41m[m
[32m+[m[32m            print(anno.bbox[box_select])[m[41m[m
[32m+[m[32m            # åœ¨å›¾ç‰‡å·¦ä¸Šè§’ï¼ˆå³ä¸‹è§’ï¼‰å’Œboxæœ€å°è¦†ç›–åŒºåŸŸæœ€å·¦ä¸Šè§’ï¼ˆå³ä¸‹è§’ï¼‰æ‰§è¡Œéšæœºè£å‰ª[m[41m[m
[32m+[m[32m            x1 = random.randint(0, left)[m[41m[m
[32m+[m[32m            y1 = random.randint(0, top)[m[41m[m
[32m+[m[32m            x2 = random.randint(right, img.shape[1])[m[41m[m
[32m+[m[32m            y2 = random.randint(bottom, img.shape[0])[m[41m[m
[32m+[m[32m            print('----- æ‰§è¡Œè£å‰ªçš„åŒºåŸŸ -----')[m[41m[m
[32m+[m[32m            print(f"{x1}, {y1}, {x2}, {y2}")[m[41m[m
[32m+[m[32m            # æ‰§è¡Œè£å‰ª[m[41m[m
[32m+[m[32m            crop = img[x1:x2, y1:y2][m[41m[m
[32m+[m[32m            # æ²¡æœ‰å®Œå…¨è¢«è£å‰ªåŒºåŸŸåŒ…æ‹¬çš„boxä¸€å¾‹åŽ»é™¤[m[41m[m
[32m+[m[32m            idx = 0[m[41m[m
[32m+[m[32m            for box in anno.bbox:[m[41m[m
[32m+[m[32m                if box[0] > x1 and box[1] > y1 and box[2] < x2 and box[3] < y2:[m[41m[m
[32m+[m[32m                    box_select[idx] = True[m[41m[m
[32m+[m[32m                idx += 1[m[41m[m
[32m+[m[32m            print('----- æœ€ç»ˆè¢«ä¿ç•™çš„box -----')[m[41m[m
[32m+[m[32m            for box_idx in range(num_box):[m[41m[m
[32m+[m[32m                if box_select[box_idx] == True:[m[41m[m
[32m+[m[32m                    print(f"box[{box_idx}]: {obj_labels[box_idx]} {anno.bbox[box_idx]}")[m[41m[m
[32m+[m[32m            # box åæ ‡éœ€è¦éšè£å‰ªå›¾ç‰‡å¤§å°è¿›è¡Œç­‰æ¯”ä¾‹è°ƒæ•´[m[41m[m
[32m+[m[32m            final_boxes = anno.bbox[box_select][m[41m[m
[32m+[m[32m            coordinate_change = torch.tensor([x1, y1, x1, y1])[m[41m[m
[32m+[m[32m            final_boxes = final_boxes - coordinate_change[m[41m[m
[32m+[m[32m            print('----- éšè£å‰ªè¿›è¡Œåæ ‡å˜æ¢åŽçš„box -----')[m[41m[m
[32m+[m[32m            print(final_boxes)[m[41m[m
[32m+[m[32m            # å¯è§†åŒ–è°ƒæ•´åŽçš„box[m[41m[m
[32m+[m[32m            # å¯¹äºŽå›¾ç‰‡ä¸­æ¯ä¸€ä¸ªbox[m[41m[m
[32m+[m[32m            print('----- è°ƒæ•´åŽçš„boxä¿¡æ¯ -----')[m[41m[m
[32m+[m[32m            for box_idx in range(len(anno.bbox)):[m[41m[m
[32m+[m[32m                if box_select[box_idx] == True:[m[41m[m
[32m+[m[32m                    box = anno.bbox[box_idx][m[41m[m
[32m+[m[32m                    x1, y1, x2, y2 = box[:4][m[41m[m
[32m+[m[32m                    x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)[m[41m[m
[32m+[m[32m                    # ä¸ºæ¯ä¸ªè¾¹æ¡†æ·»åŠ å¯¹è±¡æ ‡ç­¾[m[41m[m
[32m+[m[32m                    obj_label = str(box_idx) + ' ' + obj_labels[box_idx][m[41m[m
[32m+[m[32m                    label_size = cv2.getTextSize(obj_label + '0', cv2.FONT_HERSHEY_SIMPLEX, 0.5, 2)[0][m[41m[m
[32m+[m[32m                    if y1 - label_size[1] - 3 < 0:[m[41m[m
[32m+[m[32m                        cv2.putText(img, obj_label, (x1, y1 + label_size[1] + 3), cv2.FONT_HERSHEY_SIMPLEX,[m[41m[m
[32m+[m[32m                                    0.5, (41, 226, 243), thickness=2)[m[41m[m
[32m+[m[32m                    else:[m[41m[m
[32m+[m[32m                        cv2.putText(img, obj_label, (x1, y1 - 3), cv2.FONT_HERSHEY_SIMPLEX,[m[41m[m
[32m+[m[32m                                    0.5, (41, 226, 243), thickness=2)[m[41m[m
[32m+[m[32m                    # æ˜¾ç¤ºå¯¹è±¡è¾¹æ¡†[m[41m[m
[32m+[m[32m                    cv2.rectangle(img, (x1, y1), (x2, y2), color=box_color, thickness=2)[m[41m[m
[32m+[m[32m                    # å‘½ä»¤è¡Œè¾“å‡ºè¾¹æ¡†ä¿¡æ¯[m[41m[m
[32m+[m[32m                    print("box[{}]: {}".format(box_idx, obj_labels[box_idx]))[m[41m[m
[32m+[m[32m            # æ˜¾ç¤ºå›¾ç‰‡[m[41m[m
[32m+[m[32m            cv2.imshow("draw_1", crop)[m[41m[m
[32m+[m[32m            cv2.waitKey(0)[m[41m[m
[32m+[m[41m[m
[32m+[m[32m        # å¦‚æžœæ²¡æœ‰å…³ç³»[m[41m[m
[32m+[m[32m        else:[m[41m[m
[32m+[m[32m            print('-' * 25 + '\nno relation')[m[41m[m
[32m+[m[41m[m
[32m+[m[41m[m
[32m+[m[41m[m
[32m+[m[41m[m
 if __name__ == "__main__":[m
     # get_dataset_distribution()[m
[32m+[m[32m    clip_vg()[m[41m[m
     # visualize_vg()[m
[31m-    get_redundancy()[m
[32m+[m[32m    # get_redundancy()[m[41m[m
